{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import udf, col, lit, year, month, upper, to_date\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# setup logging \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# AWS configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('capstone.cfg', encoding='utf-8-sig')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n",
    "\n",
    "\n",
    "# data processing functions\n",
    "def create_spark_session():\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\",\\\n",
    "                \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def SAS_to_date(date):\n",
    "    if date is not None:\n",
    "        return pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "SAS_to_date_udf = udf(SAS_to_date, DateType())\n",
    "\n",
    "\n",
    "def rename_columns(table, new_columns):\n",
    "    for original, new in zip(table.columns, new_columns):\n",
    "        table = table.withColumnRenamed(original, new)\n",
    "    return table\n",
    "\n",
    "\n",
    "def process_immigration_data(spark, input_data, output_data):\n",
    "\n",
    "    logging.info(\"Start processing immigration\")\n",
    "    \n",
    "    # read immigration data file\n",
    "    immi_data = os.path.join(input_data + 'immigration/18-83510-I94-Data-2016/*.sas7bdat')\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(immi_data)\n",
    "\n",
    "    logging.info(\"Start processing fact_immigration\")\n",
    "    # extract columns to create fact_immigration table\n",
    "    fact_immigration = df.select('cicid', 'i94yr', 'i94mon', 'i94port', 'i94addr',\\\n",
    "                                 'arrdate', 'depdate', 'i94mode', 'i94visa').distinct()\\\n",
    "                         .withColumn(\"immigration_id\", monotonically_increasing_id())\n",
    "    \n",
    "    # data wrangling to match data model\n",
    "    new_columns = ['cic_id', 'year', 'month', 'city_code', 'state_code',\\\n",
    "                   'arrive_date', 'departure_date', 'mode', 'visa']\n",
    "    fact_immigration = rename_columns(fact_immigration, new_columns)\n",
    "\n",
    "    fact_immigration = fact_immigration.withColumn('country', lit('United States'))\n",
    "    fact_immigration = fact_immigration.withColumn('arrive_date', \\\n",
    "                                        SAS_to_date_udf(col('arrive_date')))\n",
    "    fact_immigration = fact_immigration.withColumn('departure_date', \\\n",
    "                                        SAS_to_date_udf(col('departure_date')))\n",
    "\n",
    "    # write fact_immigration table to parquet files partitioned by state and city\n",
    "    fact_immigration.write.mode(\"overwrite\").partitionBy('state_code')\\\n",
    "                    .parquet(path=output_data + 'fact_immigration')\n",
    "\n",
    "    logging.info(\"Start processing dim_immi_personal\")\n",
    "    # extract columns to create dim_immi_personal table\n",
    "    dim_immi_personal = df.select('cicid', 'i94cit', 'i94res',\\\n",
    "                                  'biryear', 'gender', 'insnum').distinct()\\\n",
    "                          .withColumn(\"immi_personal_id\", monotonically_increasing_id())\n",
    "    \n",
    "    # data wrangling to match data model\n",
    "    new_columns = ['cic_id', 'citizen_country', 'residence_country',\\\n",
    "                   'birth_year', 'gender', 'ins_num']\n",
    "    dim_immi_personal = rename_columns(dim_immi_personal, new_columns)\n",
    "\n",
    "    # write dim_immi_personal table to parquet files\n",
    "    dim_immi_personal.write.mode(\"overwrite\")\\\n",
    "                     .parquet(path=output_data + 'dim_immi_personal')\n",
    "\n",
    "    logging.info(\"Start processing dim_immi_airline\")\n",
    "    # extract columns to create dim_immi_airline table\n",
    "    dim_immi_airline = df.select('cicid', 'airline', 'admnum', 'fltno', 'visatype').distinct()\\\n",
    "                         .withColumn(\"immi_airline_id\", monotonically_increasing_id())\n",
    "    \n",
    "    # data wrangling to match data model\n",
    "    new_columns = ['cic_id', 'airline', 'admin_num', 'flight_number', 'visa_type']\n",
    "    dim_immi_airline = rename_columns(dim_immi_airline, new_columns)\n",
    "\n",
    "    # write dim_immi_airline table to parquet files\n",
    "    dim_immi_airline.write.mode(\"overwrite\")\\\n",
    "                    .parquet(path=output_data + 'dim_immi_airline')\n",
    "\n",
    "\n",
    "\n",
    "def process_label_descriptions(spark, input_data, output_data):\n",
    "\n",
    "\n",
    "    logging.info(\"Start processing label descriptions\")\n",
    "    label_file = os.path.join(input_data + \"I94_SAS_Labels_Descriptions.SAS\")\n",
    "    with open(label_file) as f:\n",
    "        contents = f.readlines()\n",
    "\n",
    "    country_code = {}\n",
    "    for countries in contents[10:298]:\n",
    "        pair = countries.split('=')\n",
    "        code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "        country_code[code] = country\n",
    "    spark.createDataFrame(country_code.items(), ['code', 'country'])\\\n",
    "         .write.mode(\"overwrite\")\\\n",
    "         .parquet(path=output_data + 'country_code')\n",
    "\n",
    "    city_code = {}\n",
    "    for cities in contents[303:962]:\n",
    "        pair = cities.split('=')\n",
    "        code, city = pair[0].strip(\"\\t\").strip().strip(\"'\"),\\\n",
    "                     pair[1].strip('\\t').strip().strip(\"''\")\n",
    "        city_code[code] = city\n",
    "    spark.createDataFrame(city_code.items(), ['code', 'city'])\\\n",
    "         .write.mode(\"overwrite\")\\\n",
    "         .parquet(path=output_data + 'city_code')\n",
    "\n",
    "    state_code = {}\n",
    "    for states in contents[982:1036]:\n",
    "        pair = states.split('=')\n",
    "        code, state = pair[0].strip('\\t').strip(\"'\"), pair[1].strip().strip(\"'\")\n",
    "        state_code[code] = state\n",
    "    spark.createDataFrame(state_code.items(), ['code', 'state'])\\\n",
    "         .write.mode(\"overwrite\")\\\n",
    "         .parquet(path=output_data + 'state_code')\n",
    "\n",
    "\n",
    "\n",
    "def process_temperature_data(spark, input_data, output_data):\n",
    "\n",
    "    logging.info(\"Start processing dim_temperature\")\n",
    "    # read temperature data file\n",
    "    tempe_data = os.path.join(input_data + 'temperature/GlobalLandTemperaturesByCity.csv')\n",
    "    df = spark.read.csv(tempe_data, header=True)\n",
    "\n",
    "    df = df.where(df['Country'] == 'United States')\n",
    "    dim_temperature = df.select(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty',\\\n",
    "                         'City', 'Country']).distinct()\n",
    "\n",
    "    new_columns = ['dt', 'avg_temp', 'avg_temp_uncertnty', 'city', 'country']\n",
    "    dim_temperature = rename_columns(dim_temperature, new_columns)\n",
    "\n",
    "    dim_temperature = dim_temperature.withColumn('dt', to_date(col('dt')))\n",
    "    dim_temperature = dim_temperature.withColumn('year', year(dim_temperature['dt']))\n",
    "    dim_temperature = dim_temperature.withColumn('month', month(dim_temperature['dt']))\n",
    " \n",
    "    # write dim_temperature table to parquet files\n",
    "    dim_temperature.write.mode(\"overwrite\")\\\n",
    "                   .parquet(path=output_data + 'dim_temperature')\n",
    "\n",
    "\n",
    "\n",
    "def process_demography_data(spark, input_data, output_data):\n",
    " \n",
    "    logging.info(\"Start processing dim_demog_populaiton\")\n",
    "    # read demography data file\n",
    "    demog_data = os.path.join(input_data + 'demography/us-cities-demographics.csv')\n",
    "    df = spark.read.format('csv').options(header=True, delimiter=';').load(demog_data)\n",
    "\n",
    "\n",
    "    dim_demog_population = df.select(['City', 'State', 'Male Population', 'Female Population', \\\n",
    "                              'Number of Veterans', 'Foreign-born', 'Race']).distinct() \\\n",
    "                              .withColumn(\"demog_pop_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "    new_columns = ['city', 'state', 'male_population', 'female_population', \\\n",
    "                   'num_vetarans', 'foreign_born', 'race']\n",
    "    dim_demog_population = rename_columns(dim_demog_population, new_columns)\n",
    "\n",
    "    # write dim_demog_population table to parquet files\n",
    "    dim_demog_population.write.mode(\"overwrite\")\\\n",
    "                        .parquet(path=output_data + 'dim_demog_population')\n",
    "\n",
    "    \n",
    "    logging.info(\"Start processing dim_demog_statistics\")\n",
    "    dim_demog_statistics = df.select(['City', 'State', 'Median Age', 'Average Household Size'])\\\n",
    "                             .distinct()\\\n",
    "                             .withColumn(\"demog_stat_id\", monotonically_increasing_id())\n",
    "\n",
    "    new_columns = ['city', 'state', 'median_age', 'avg_household_size']\n",
    "    dim_demog_statistics = rename_columns(dim_demog_statistics, new_columns)\n",
    "    dim_demog_statistics = dim_demog_statistics.withColumn('city', upper(col('city')))\n",
    "    dim_demog_statistics = dim_demog_statistics.withColumn('state', upper(col('state')))\n",
    "\n",
    "    # write dim_demog_statistics table to parquet files\n",
    "    dim_demog_statistics.write.mode(\"overwrite\")\\\n",
    "                        .parquet(path=output_data + 'dim_demog_statistics')\n",
    "\n",
    "\n",
    "    \n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = SOURCE_S3_BUCKET\n",
    "    output_data = DEST_S3_BUCKET\n",
    "    \n",
    "    process_immigration_data(spark, input_data, output_data)    \n",
    "    process_label_descriptions(spark, input_data, output_data)\n",
    "    process_temperature_data(spark, input_data, output_data)\n",
    "    process_demography_data(spark, input_data, output_data)\n",
    "    logging.info(\"Data processing completed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
